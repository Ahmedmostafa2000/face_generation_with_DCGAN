{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7BxRdy9M5DIu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\program_files\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "d:\\program_files\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import Sequential,optimizers\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.layers import Input, Dense,BatchNormalization,Conv2DTranspose,Conv2D,Dropout,Flatten, Reshape, Activation \n",
        "from tensorflow_addons.layers import GroupNormalization\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "import pickle\n",
        "\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOISE_DIM = 100\n",
        "BATCH_SIZE = 32\n",
        "INSTANCE_NUM = 10000\n",
        "BATCHES = INSTANCE_NUM//BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UjAr7Ejp5h7v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16384)             1638400   \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 16, 16, 64)        65664     \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 32, 32, 32)        32832     \n",
            "                                                                 \n",
            " sequential_3 (Sequential)   (None, 64, 64, 16)        8224      \n",
            "                                                                 \n",
            " sequential_4 (Sequential)   (None, 128, 128, 1)       256       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,745,376\n",
            "Trainable params: 1,745,376\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def convTBlock(output_channels,kernal_size, strides, padding='same', Inst_Norm=True, activation = \"relu\"):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2DTranspose(output_channels, kernal_size, strides=strides, padding=padding, use_bias=False,kernel_regularizer=tf.keras.regularizers.l2(0.0005)))\n",
        "    if Inst_Norm:\n",
        "        model.add(GroupNormalization(groups = 1))\n",
        "\n",
        "    model.add(Activation(activation))\n",
        "    return model\n",
        "\n",
        "def generator_model(noise_dim):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(2048*8, use_bias=False, input_shape=(noise_dim,)))\n",
        "    model.add(Reshape((16,16 , 64)))\n",
        "    \n",
        "    model.add(convTBlock(64     , 4, 1))\n",
        "    model.add(convTBlock(32     , 4, 2))\n",
        "    model.add(convTBlock(16     , 4, 2))\n",
        "    model.add(convTBlock(1      , 4, 2, Inst_Norm=False, activation = \"leaky_relu\"))\n",
        "    \n",
        "    return model\n",
        "\n",
        "generator = generator_model(NOISE_DIM)\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xBowmbrC7sgk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_6 (Sequential)   (None, 64, 64, 16)        256       \n",
            "                                                                 \n",
            " sequential_7 (Sequential)   (None, 32, 32, 32)        8192      \n",
            "                                                                 \n",
            " sequential_8 (Sequential)   (None, 16, 16, 64)        32768     \n",
            "                                                                 \n",
            " sequential_9 (Sequential)   (None, 8, 8, 128)         131072    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 180,481\n",
            "Trainable params: 180,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def convBlock(output_channels,kernal_size, strides, padding='same', batch_normalization=False, activation = \"leaky_relu\",dropout = 0.2):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(output_channels, kernal_size, strides=strides, padding=padding, use_bias=False,kernel_regularizer=tf.keras.regularizers.l2(0.0005)))\n",
        "    if batch_normalization:\n",
        "        model.add(BatchNormalization())\n",
        "    model.add(Activation(activation))\n",
        "    if dropout:\n",
        "        model.add(Dropout(dropout))\n",
        "    return model\n",
        "\n",
        "def discriminator_model():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=[128, 128, 1]))\n",
        "\n",
        "    model.add(convBlock(16,  4, 2, padding='same', dropout=0.3))\n",
        "    model.add(convBlock(32,  4, 2, padding='same', dropout=0.3))\n",
        "    model.add(convBlock(64, 4, 2, padding='same', dropout=0.3))\n",
        "    model.add(convBlock(128,4, 2, padding='same', batch_normalization=False))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1,activation =\"sigmoid\"))\n",
        "\n",
        "    return model\n",
        "discriminator = discriminator_model()\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-WWGt2kh7n9",
        "outputId": "54a90858-8c21-4608-c201-8aadd102f07f"
      },
      "outputs": [],
      "source": [
        "with open(\"data.pkl\", \"rb\") as file:\n",
        "    df = pickle.load(file)[:INSTANCE_NUM]\n",
        "df = (np.asarray(df, dtype = np.float16)).reshape(-1, 128, 128,1).astype(np.float16)\n",
        "df = df/255\n",
        "dataset = tf.data.Dataset.from_tensor_slices(df)\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "epsilon = 1e-8\n",
        "def discRealLoss(real_output):\n",
        "    return -tf.reduce_mean(tf.math.log(real_output + epsilon))\n",
        "def discFakeLoss(fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(1 - fake_output + epsilon))\n",
        "def genRealLoss(fake_output):\n",
        "    return -tf.reduce_mean(tf.math.log(fake_output + epsilon))\n",
        "\n",
        "class CustomDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_learning_rate, decay_steps, decay_rate, min_learning_rate):\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_rate = decay_rate\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Calculate the decayed learning rate\n",
        "        decayed_learning_rate = self.initial_learning_rate * (self.decay_rate ** (step // self.decay_steps))\n",
        "        # Ensure the learning rate does not go below the minimum\n",
        "        return tf.maximum(decayed_learning_rate, self.min_learning_rate)\n",
        "    \n",
        "lr_schedule = CustomDecaySchedule(\n",
        "    1e-4,\n",
        "    decay_steps=BATCHES//100,\n",
        "    decay_rate=0.96,\n",
        "    min_learning_rate=1e-6)\n",
        "\n",
        "\n",
        "gen_opt= optimizers.Adam(\n",
        "    learning_rate=lr_schedule,\n",
        "    )\n",
        "dis_opt = optimizers.Adam(\n",
        "    learning_rate=1e-4,\n",
        "    )\n",
        "\n",
        "def show_info(batch_number, epoch,epochs, fake_loss, dis_loss, start):\n",
        "    clear_output(wait=True)\n",
        "    display(f\"epoch: {epoch}/{epochs} | batch: {batch_number}/{BATCHES}\")\n",
        "    display(f\"gen_loss: {fake_loss:.4f}\")\n",
        "    display(f\"disc_loss: {dis_loss:.4f}\")\n",
        "    display(f\"time: {time.time() - start:.2f}s\") \n",
        "\n",
        "log_dir = \"log/\"+time.strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
        "summary_writer = tf.summary.create_file_writer(log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_discriminator(images, generated_images):\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        real = discriminator(images, training=True)\n",
        "        fake = discriminator(generated_images, training=True)\n",
        "        \n",
        "        real_loss = discRealLoss(real)\n",
        "        fake_loss = discFakeLoss(fake)\n",
        "        disc_loss = real_loss + fake_loss\n",
        "    \n",
        "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    dis_opt.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "    return disc_loss,real,fake\n",
        "\n",
        "@tf.function\n",
        "def train_generator(noise):\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        fake = discriminator(generated_images, training=True)\n",
        "        gen_loss = genRealLoss(real,fake)\n",
        "    \n",
        "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gen_opt.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "    return gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "constant_noise = tf.random.normal([BATCH_SIZE, NOISE_DIM],seed=1)\n",
        "last_loss = math.inf\n",
        "from filelock import FileLock\n",
        "\n",
        "def train_epoch(images, batch_size, noise_dim,epoch,batch_number,epochs):\n",
        "    global last_loss\n",
        "    \n",
        "    start = time.time()\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "\n",
        "    generated = generator(noise, training=True)\n",
        "\n",
        "    disc_loss,real,fake = train_discriminator(images, generated)\n",
        "    gen_loss = train_generator(noise)\n",
        "    \n",
        "\n",
        "    show_info(batch_number, epoch,epochs, gen_loss, disc_loss, start)\n",
        "    \n",
        "    with summary_writer.as_default():\n",
        "        tf.summary.scalar('Loss/gen', gen_loss,     step=(INSTANCE_NUM*(epoch-1)//BATCH_SIZE)+batch_number)\n",
        "        tf.summary.scalar('Loss/dis' , disc_loss,    step=(INSTANCE_NUM*(epoch-1)//BATCH_SIZE)+batch_number)\n",
        "        tf.summary.scalar('Loss/real' , int(sum(real.numpy()>.5)),    step=(INSTANCE_NUM*(epoch-1)//BATCH_SIZE)+batch_number)\n",
        "        tf.summary.scalar('Loss/fake' , int(sum(fake.numpy()>.5)),    step=(INSTANCE_NUM*(epoch-1)//BATCH_SIZE)+batch_number)\n",
        "        \n",
        "    if gen_loss < last_loss:\n",
        "        generator.save(\"generator.keras\")\n",
        "        discriminator.save(\"discriminator.keras\")\n",
        "        last_loss= gen_loss\n",
        "        \n",
        "    lock = FileLock(\"image.pkl.lock\")\n",
        "    with lock:\n",
        "        with open(\"image.pkl\", \"wb\") as file:\n",
        "            pickle.dump([epoch,batch_number,np.asarray(generator(constant_noise)[0])], file)\n",
        "    \n",
        "def train(dataset, epochs= 20,batch_size=32):\n",
        "    start = time.time()\n",
        "    for epoch_number in range(epochs):\n",
        "        batch_number = 1\n",
        "        for batch in dataset:\n",
        "            batch_number+=1\n",
        "            train_epoch(images = batch,\n",
        "                        batch_size = batch_size,\n",
        "                        noise_dim=NOISE_DIM,\n",
        "                        epoch=epoch_number+1,\n",
        "                        batch_number=batch_number,\n",
        "                        epochs = epochs)\n",
        "            display(f\"Total time: {time.time() - start:.2f}s\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jvx-MxLny_yH",
        "outputId": "980360e5-7127-4165-9aad-a34dfbd630cb"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n    File \"C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_6748\\2480240856.py\", line 20, in train_generator  *\n        gen_loss = genRealLoss(fake)\n\n    TypeError: tf__genRealLoss() missing 1 required positional argument: 'fake_output'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManually Interrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs, batch_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m     41\u001b[0m     batch_number\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNOISE_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_number\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                \u001b[49m\u001b[43mbatch_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     display(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(images, batch_size, noise_dim, epoch, batch_number, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m generated \u001b[38;5;241m=\u001b[39m generator(noise, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m disc_loss,real,fake \u001b[38;5;241m=\u001b[39m train_discriminator(images, generated)\n\u001b[1;32m---> 15\u001b[0m gen_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m show_info(batch_number, epoch,epochs, gen_loss, disc_loss, start)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summary_writer\u001b[38;5;241m.\u001b[39mas_default():\n",
            "File \u001b[1;32md:\\program_files\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileheiyk_3r.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_generator\u001b[1;34m(noise)\u001b[0m\n\u001b[0;32m     11\u001b[0m     generated_images \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator), (ag__\u001b[38;5;241m.\u001b[39mld(noise),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[0;32m     12\u001b[0m     fake \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(generated_images),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)\n\u001b[1;32m---> 13\u001b[0m     gen_loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(genRealLoss), (ag__\u001b[38;5;241m.\u001b[39mld(fake),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m gen_gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gen_tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(gen_loss), ag__\u001b[38;5;241m.\u001b[39mld(generator)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gen_opt)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gen_gradients), ag__\u001b[38;5;241m.\u001b[39mld(generator)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
            "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_6748\\2480240856.py\", line 20, in train_generator  *\n        gen_loss = genRealLoss(fake)\n\n    TypeError: tf__genRealLoss() missing 1 required positional argument: 'fake_output'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    train(dataset, 50, BATCH_SIZE)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Manually Interrupted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
